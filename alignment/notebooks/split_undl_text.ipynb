{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75b0321b-c721-4d5e-863d-df58e1ae5cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name_map = {\n",
    "    \"de\": \"Helsinki-NLP/opus-mt-de-en\",\n",
    "    \"zh\": \"Helsinki-NLP/opus-mt-zh-en\",\n",
    "    \"fr\": \"Helsinki-NLP/opus-mt-fr-en\",\n",
    "    \"ar\": \"Helsinki-NLP/opus-mt-ar-en\",\n",
    "    \"ru\": \"Helsinki-NLP/opus-mt-ru-en\",\n",
    "    \"es\": \"Helsinki-NLP/opus-mt-es-en\"\n",
    "}\n",
    "\n",
    "model_map = {}\n",
    "\n",
    "for lang, model in model_name_map.items():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    model_map[lang] = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b2a0d81a-3640-4ad6-8340-41a94b818189",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_symbols_map = {\n",
    "    \"zh\": ['\\n', '。', '；', ':', ',', ' '],\n",
    "    \"de\": ['\\n', '.', ';', ':', ',', ' '],\n",
    "    \"fr\": ['\\n', '.', ';', ':', ',', ' '],\n",
    "    \"ar\": ['\\n', '،', ';', '.', ':', ' '],\n",
    "    \"ru\": ['\\n', '.', ';', ':', ',', ' '],\n",
    "    \"es\": ['\\n', '.', ';', ':', ',', ' '],\n",
    "}\n",
    "\n",
    "def split_sentence(sentence, tokenizer, lang, token_limit=512):\n",
    "    \"\"\"\n",
    "    将一个句子分成若干块，每块的token数量不超过token_limit。\n",
    "    尝试使用不同的分隔符来拆分句子。\n",
    "    \"\"\"\n",
    "    split_symbols = split_symbols_map.get(lang)\n",
    "\n",
    "    for split_symbol in split_symbols:\n",
    "        chunks = sentence.split(split_symbol)\n",
    "\n",
    "        if len(chunks) == 1:\n",
    "            continue\n",
    "\n",
    "        chunked_results = []\n",
    "        current_content = \"\"\n",
    "        current_tokens = 0\n",
    "\n",
    "        for chunk in chunks:\n",
    "            tokens = tokenizer.encode(chunk)\n",
    "            added_tokens = len(tokens) + (len(tokenizer.encode(split_symbol)) if current_content else 0)\n",
    "    \n",
    "            # 如果当前块与新的块合并后不超过token限制，则合并它们。\n",
    "            if current_tokens + added_tokens <= token_limit:\n",
    "                current_content += (split_symbol if current_content else \"\") + chunk\n",
    "                current_tokens += added_tokens\n",
    "            else:\n",
    "                # 否则，保存当前块并开始一个新的块。\n",
    "                if current_content:\n",
    "                    chunked_results.append({\"content\": current_content, \"token_total\": tokenizer.encode(current_content), \"split_symbol\": split_symbol})\n",
    "                current_content = chunk\n",
    "                current_tokens = len(tokens)\n",
    "    \n",
    "        # 添加最后一个块。\n",
    "        if current_content:\n",
    "            chunked_results.append({\"content\": current_content, \"token_total\": tokenizer.encode(current_content), \"split_symbol\": split_symbol})\n",
    "    \n",
    "        # 确保所有块的大小都不超过token限制。\n",
    "        if chunked_results and all([len(chunk[\"token_total\"]) <= token_limit for chunk in chunked_results]):\n",
    "            return chunked_results\n",
    "    \n",
    "    return [{\"content\": sentence, \"token_total\": len(tokenizer.encode(sentence)), \"split_symbol\": \"\"}]\n",
    "\n",
    "def split_text_into_chunks(text, tokenizer, lang, token_limit=512):\n",
    "    \"\"\"\n",
    "    将文本分成若干块，每块的token数量不超过token_limit。\n",
    "    这是通过首先按段落拆分文本，然后进一步拆分长段落来实现的。\n",
    "    \"\"\"\n",
    "    \n",
    "    paragraphs = text.split(\"\\n\\n\")\n",
    "    results = []\n",
    "\n",
    "    for index, para in enumerate(paragraphs):\n",
    "        tokens = tokenizer.encode(para)\n",
    "        \n",
    "        # 如果段落的token数量小于token限制，直接添加到结果中。\n",
    "        if len(tokens) < token_limit:\n",
    "            results.append({\n",
    "                \"id\": str(index),\n",
    "                \"content\": para,\n",
    "                \"token_total\": len(tokens),\n",
    "                \"split_symbol\": \"\\n\\n\"\n",
    "            })\n",
    "        else:\n",
    "            # 否则，进一步拆分段落。\n",
    "            chunks = split_sentence(para, tokenizer, lang, token_limit)\n",
    "            for idx, chunk in enumerate(chunks):\n",
    "                results.append({\n",
    "                    \"id\": f\"{index}-{idx}\",\n",
    "                    \"content\": chunk[\"content\"],\n",
    "                    \"token_total\": len(chunk[\"token_total\"]),\n",
    "                    \"split_symbol\": chunk[\"split_symbol\"]\n",
    "                })\n",
    "\n",
    "    return results\n",
    "\n",
    "def reconstruct_text(chunks):\n",
    "    reconstructed = []\n",
    "    current_paragraph = []\n",
    "    current_parent_id = None\n",
    "\n",
    "    for chunk in chunks:\n",
    "        # Split the ID to determine the parent ID and potential sub-IDs\n",
    "        id_split = chunk[\"id\"].split(\"-\")\n",
    "        parent_id = id_split[0]\n",
    "\n",
    "        # If this is a new paragraph, append the previous paragraph to the results and start a new one\n",
    "        if parent_id != current_parent_id:\n",
    "            if current_paragraph:\n",
    "                reconstructed.append(current_paragraph)\n",
    "            current_paragraph = []\n",
    "            current_parent_id = parent_id\n",
    "\n",
    "        current_paragraph.append(chunk)\n",
    "\n",
    "    # Append the last paragraph if there's any content left\n",
    "    if current_paragraph:\n",
    "        reconstructed.append(current_paragraph)\n",
    "\n",
    "    completed_reconstructed = []\n",
    "    for arr_para in reconstructed:\n",
    "        if len(arr_para) == 1:\n",
    "            completed_reconstructed.append(arr_para[0][\"content\"])\n",
    "        else:\n",
    "            content = \"\"\n",
    "            for para in arr_para:\n",
    "                content += (para[\"split_symbol\"] if content else \"\") + para[\"content\"]\n",
    "            completed_reconstructed.append(content)\n",
    "\n",
    "    return completed_reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5905ece1-beeb-4b10-b20e-2af228355631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import json\n",
    "\n",
    "def clean_paragraph(paragraph):\n",
    "    lines = paragraph.split('\\n')\n",
    "    para = ''\n",
    "    table = []\n",
    "    discarded = []  # 用于保存被舍弃的内容\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        \n",
    "        # 表格线或其他分割线\n",
    "        if re.match(r'^(\\+[-=+]+\\+|-+|=+|_+)$', line):\n",
    "            discarded.append(line)\n",
    "            if not para.endswith('\\n'):\n",
    "                para += '\\n'\n",
    "            if len(table) > 0:\n",
    "                para += '\\t'.join(table)\n",
    "                table = []\n",
    "                \n",
    "        # 表格中的空行\n",
    "        elif re.match(r'^\\|( +\\|)+$', line):\n",
    "            discarded.append(line)\n",
    "            para += '\\t'.join(table) + ' '\n",
    "            table = []\n",
    "            \n",
    "        # 表格中的内容行\n",
    "        elif re.match(r'^\\|([^|]+\\|)+$', line):\n",
    "            if len(table) == 0:\n",
    "                table = line[1:-2].split('|')\n",
    "            else:\n",
    "                arr = line[1:-2].split('|')\n",
    "                if len(arr) == len(table):\n",
    "                    table = [table[i].strip() + arr[i].strip() for i in range(len(table))]\n",
    "                elif len(arr) > len(table):\n",
    "                    table = [table[i].strip() + arr[i].strip() if i < len(table) else arr[i].strip() for i in range(len(arr))]\n",
    "                else:\n",
    "                    table = [table[i].strip() + arr[i].strip() if i < len(arr) else table[i].strip() for i in range(len(table))]\n",
    "        # 正文内容\n",
    "        else:\n",
    "            para += ' ' + line\n",
    "\n",
    "    if len(table) > 0:\n",
    "        if not para.endswith('\\n'):\n",
    "            para += '\\n'\n",
    "            \n",
    "        para += '\\t'.join(table)\n",
    "        \n",
    "    # # 如果有被舍弃的内容，保存到文件\n",
    "    # if discarded:\n",
    "    #     with open(\"discarded_content.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
    "    #         f.write(json.dumps({\"discarded\": discarded, \"method\": \"clean_table\"}) + \"\\n\")\n",
    "        \n",
    "    return re.sub(r'[ \\t]{2,}', ' ', re.sub(r'\\n{2,}', '\\n', para)).strip()\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    cleaned_paragraphs = [clean_paragraph(para) for para in text.split(\"\\n\\n\")]\n",
    "    cleaned_paragraphs = list(filter(lambda x: x.strip() != \"\" and x != \"[]\", cleaned_paragraphs))\n",
    "    \n",
    "    processed_paragraphs = []\n",
    "    for para in cleaned_paragraphs:\n",
    "        lines = para.splitlines()\n",
    "\n",
    "        # 替换那些\"-\"出现频率超过30%的行中的\"-\"字符为空格\n",
    "        for idx, line in enumerate(lines):\n",
    "            num_dashes = line.count(\"-\")\n",
    "            if num_dashes / len(line) >= 0.2:\n",
    "                modified_line = line.replace(\"-\", \" \")\n",
    "                \n",
    "                if modified_line.strip() == \"\":\n",
    "                    del lines[idx]\n",
    "                else:\n",
    "                    lines[idx] = modified_line\n",
    "                    idx += 1\n",
    "        \n",
    "                # # Save the count of '-' replaced in the line\n",
    "                # with open(\"discarded_content.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
    "                #     f.write(json.dumps({\"discarded\": \"-\" * num_dashes, \"method\": \"replace '-' with space\"}) + \"\\n\")\n",
    "\n",
    "        processed_paragraphs.append(\"\\n\".join(lines))\n",
    "        \n",
    "    return \"\\n\\n\".join(filter(lambda para: para.strip() != \"\", processed_paragraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3995b9c4-7eca-402f-8ef2-fce8c4404319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def process_row(row):\n",
    "    results = []\n",
    "    \n",
    "    record = row[\"record\"]\n",
    "    for lang, content in row.items():\n",
    "   \n",
    "        if lang in [\"en\", \"record\"]:\n",
    "            continue\n",
    "\n",
    "        if content.strip() == \"\":\n",
    "            continue\n",
    "        \n",
    "        processed_text = preprocess(content)\n",
    "        tokenizer = model_map[lang]\n",
    "        chunks = split_text_into_chunks(processed_text, tokenizer, lang)\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            chunk.update({\"record\": record, \"source\": lang, \"target\": \"en\"})\n",
    "            results.append(chunk)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2caa48e0-c179-47ac-95b2-4c0d3fe1b3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_items_generator(data_list):\n",
    "    seen_contents = set()\n",
    "    for item in data_list:\n",
    "        if item['content'] not in seen_contents:\n",
    "            yield item\n",
    "            \n",
    "        if len(item['content']) < 10:\n",
    "            seen_contents.add(item['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "27bbb3f2-94b5-4549-a94a-81d6a10c6396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36da5bc498054423bf225c9398056fa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.load_dataset(\"bot-yaya/undl_text\", split=\"train[:100]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "894975fd-a28e-4452-8cfb-bbb7eec6ef0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[AToken indices sequence length is longer than the specified maximum sequence length for this model (748 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (887 > 512). Running this sequence through the model will result in indexing errors\n",
      "\n",
      "\n",
      "9it [00:00, 71.71it/s]\u001b[A\u001b[AToken indices sequence length is longer than the specified maximum sequence length for this model (1009 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (955 > 512). Running this sequence through the model will result in indexing errors\n",
      "\n",
      "\n",
      "17it [00:00, 18.80it/s]\u001b[A\u001b[AToken indices sequence length is longer than the specified maximum sequence length for this model (610 > 512). Running this sequence through the model will result in indexing errors\n",
      "\n",
      "\n",
      "21it [00:01, 15.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "24it [00:01, 13.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "27it [00:01, 14.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "30it [00:01, 13.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "32it [00:02, 12.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "34it [00:02, 12.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "36it [00:02, 12.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "38it [00:03,  5.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "41it [00:03,  7.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "43it [00:03,  8.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "46it [00:03, 11.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "48it [00:05,  4.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "50it [00:05,  4.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "52it [00:05,  5.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "54it [00:06,  4.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "55it [00:06,  4.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "59it [00:06,  7.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "61it [00:06,  8.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "67it [00:06, 15.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "73it [00:06, 22.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "77it [00:07, 24.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "81it [00:07, 17.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "84it [00:07, 19.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "89it [00:07, 24.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "93it [00:07, 23.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "97it [00:08, 18.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "100it [00:08, 12.05it/s]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "# with Pool(8) as p:\n",
    "#     result_total = list(tqdm(p.imap(process_row, dataset), total=len(dataset)))\n",
    "\n",
    "# flattened_result = [item for sublist in result_total for item in sublist]\n",
    "\n",
    "results = []\n",
    "for index, row in tqdm(enumerate(dataset)):\n",
    "    results += process_row(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "36235747-5557-4d20-8b45-e352d74bc8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'content', 'token_total', 'split_symbol', 'record', 'source', 'target'],\n",
       "    num_rows: 68352\n",
       "})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Features, Value\n",
    "\n",
    "features = Features({\n",
    "    'id': Value('string'),\n",
    "    'content': Value('string'),\n",
    "    'token_total': Value('int32'),\n",
    "    'split_symbol': Value('string'),\n",
    "    'record': Value('string'),\n",
    "    'source': Value('string'),\n",
    "    'target': Value('string'),\n",
    "})\n",
    "\n",
    "def generator_wrapper(data_to_process):\n",
    "    def generator():\n",
    "        return unique_items_generator(data_to_process)\n",
    "    return generator\n",
    "\n",
    "new_dataset = datasets.Dataset.from_generator(generator_wrapper(results), features=features)\n",
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "96d67351-4545-4eb7-b1b0-0d2a7bcea265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'content', 'token_total', 'split_symbol', 'record', 'source', 'target'],\n",
       "    num_rows: 6\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = new_dataset.filter(lambda row: row[\"token_total\"] > 500)\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2397e2a4-bbc4-4e26-8e57-9799d809b2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': ['118-0', '93', '340', '76'],\n",
       " 'content': ['Статья 10. Обязательствораскрывать информации\\t\\nДля арбитров и кандидатов варбитры\\tДля судей и кандидатов надолжность судьи\\n1. Кандидат и арбитр раскрываютинформацию о любыхобстоятельствах, которые могутвызвать оправданные сомнения[,\\xa0в\\xa0том числе с точки зрениясторон в споре,] в ихнезависимости илибеспристрастности.\\t<То же> за исключением оговорки«с точки зрения сторон в споре»,которая не будет применяться ксудьям и кандидатам на должностьсудьи2. Раскрываются, в частности,следующие сведения о:\\t*Судья, раскрывает, в частности,сведения\\xa0о: a) любых финансовых, деловых,профессиональных и личныхотношениях за последние пять летс:\\ta) любых финансовых, деловых,профессиональных и личныхотношениях за последние пять летс: i) любой из сторон в споре илиорганизацией, указанной сторонойв споре;\\ti) любой из сторон в споре илиорганизацией, указанной сторонойв споре; ii) законным(-ыми)представителем(-ями) любой изсторон в рамках разбирательствапо МИС;\\tii) законным(-ыми)представителем(-ями) любой изсторон спора в рамкахразбирательства по МИС; \\niii) другими арбитрами исвидетелями-экспертами в рамкахразбирательства по МИС; иiv) [любой организацией, котораяуказана стороной в споре вкачестве стороны, имеющей прямуюили косвенную заинтересованностьв исходе разбирательства по МИС,включая третьи стороны,осуществляющие финансирование];b) любой финансовой или личнойзаинтересованности в:i) исходе разбирательства по МИС;\\tiii) экспертами-свидетелями врамках разбирательства по МИС; иiv) [любой организацией, котораяуказана стороной в споре вкачестве стороны, имеющей прямуюили косвенную заинтересованностьв исходе разбирательства по МИС,включая третьи стороны,осуществляющие финансирование];b) любой финансовой или личнойзаинтересованности в:i) исходе разбирательства поМИС;ii) любом другом разбирательствепо МИС, связанном с той (теми) жемерой(-ами); и\\tii) любом другом разбирательствепо МИС, связанном с той (теми)же мерой(-ами); и \\niii) любом другом разбирательствес участием одной из сторон вспоре или организаций, указанныхстороной в споре;\\tiii) [не применяется]',\n",
       "  '61. Au niveau opérationnel, l’ONUDC a continué de coordonner ses activités et de coopérer avec les organismes et bureaux des Nations Unies concernés, comme le Programme des Nations Unies pour le développement (PNUD), le Centre de documentation d’Europe du Sud-Est et de l’Est pour le contrôle des armes légères et de petit calibre, le Bureau des affaires de désarmement et ses centres régionaux (notamment le Centre régional des Nations Unies pour la paix et le désarmement en Afrique et le Centre régional des Nations Unies pour la paix, le désarmement et le développement en Amérique latine et dans les Caraïbes), ainsi que le Département des opérations de maintien de la paix, y compris le Service de la lutte antimines. Dans les Balkans occidentaux, l’ONUDC et le PNUD ont créé un fonds d’affectation spéciale multipartenaires pour appuyer la mise en œuvre de la feuille de route pour une solution durable à la détention illégale, au mauvais usage et au trafic d’armes légères et de petit calibre d’ici à 2024. Ce fonds a permis à plusieurs projets mis en œuvre dans la région, dont quatre de l’ONUDC, de recevoir le soutien de donateurs. Dans les Caraïbes, l’ONUDC appuie le volet «\\xa0justice pénale\\xa0» de la feuille de route caribéenne relative aux armes à feu, que coordonnent le Centre régional des Nations Unies pour la paix, le désarmement et le développement en Amérique latine et dans les Caraïbes et la CARICOM\\xa0; il a également lancé, en juin 2022, dans le cadre du fonds «\\xa0Sauver des vies\\xa0», en coopération avec le PNUD, le Centre régional des Nations Unies pour la paix, le désarmement et le développement en Amérique latine et dans les Caraïbes et l’Organisation des Nations Unies pour l’éducation, la science et la culture (UNESCO), un projet interinstitutions destiné à réduire le trafic d’armes à feu en Jamaïque. En outre, l’ONUDC a poursuivi sa coopération avec le Centre des Nations Unies pour la lutte contre le terrorisme et le Bureau de lutte contre le terrorisme dans le cadre de la mise en œuvre de la deuxième phase de son projet conjoint qui vise à s’attaquer aux liens qui existent entre le terrorisme et le trafic d’armes à feu en Asie centrale, favorisant une approche unique des Nations Unies, en coopération également avec la Direction exécutive du Comité contre le terrorisme et le Bureau des affaires de désarmement. En collaboration avec l’UNIDIR, l’ONUDC travaille à l’élaboration d’une publication conjointe sur le lien qui existe entre les armes, la criminalité et les conflits.',\n",
       "  '233. Si la controversia no puede resolverse de manera amistosa y exige una resolución más formal, el arbitraje puede ser una opción preferible a la vía judicial, en particular en controversias por préstamos de escasa cuantía. En algunos países, se incluyen en el contrato de préstamo cláusulas que obligan a someter a arbitraje las controversias que surjan entre financiadores y MIPYME, si bien esto puede limitar las opciones de reparación de las MIPYME[479]. Aunque el arbitraje es menos formal que un procedimiento judicial (y, por ende, menos intimidatorio para una pequeña empresa), sigue normas procesales más simples que se pueden adaptar a las necesidades de las partes y es flexible en cuanto al calendario y la ubicación de las audiencias, se trata en cualquier caso de un procedimiento contencioso que obliga a contar con abogados y, en ocasiones, con otros expertos. Por ello, puede resultar una opción cara y no viable para una MIPYME, en particular dada la escasa cuantía del préstamo. Además, normalmente solo se puede interponer un recurso contra el laudo arbitral ante los tribunales. Los textos legislativos y contractuales de la CNUDMI en materia de arbitraje comercial internacional establecen un conjunto completo de normas que ayudan i) a los Estados a reforzar su régimen interno en materia de arbitraje y ii) a las instituciones arbitrales a redactar normas que regulen los procesos arbitrales. A fin de mejorar el acceso de las MIPYME a los mecanismos de solución de controversias, las organizaciones regionales y los Estados apoyan cada vez más el uso de mecanismos de solución de controversias en línea. Son plataformas fáciles de usar, rápidas y de bajo costo que no requieren la comparecencia física de las partes. Estas y otras características los hacen especialmente adecuados para las controversias de escasa cuantía y las derivadas de operaciones transfronterizas. Por ejemplo, el Foro de Cooperación Económica de Asia y el Pacífico (APEC) ha puesto en marcha el Marco de Colaboración para la Solución En Línea de Controversias Transfronterizas entre Empresas con el objetivo de ayudar a las pequeñas empresas a dirimir las controversias transfronterizas de escasa cuantía. Estos mecanismos requieren un entorno jurídico propicio que permita, por ejemplo, la elección del foro y que no exija la comparecencia física de las partes o la presentación física de documentos por escrito. Por lo tanto, los Estados pueden tener que modificar la legislación nacional en consecuencia. Las Notas técnicas de la CNUDMI sobre la solución de controversias en línea (2017) pueden proporcionar orientación a los Estados, a las plataformas para la solución de controversias en línea y a los administradores sobre el modo de concebir y utilizar esos mecanismos[480].',\n",
       "  '51. La UNODC también había elaborado una serie de productos del conocimiento, entre los que se contaban el informe mundial sobre la corrupción en el deporte (Global Report on Corruption in Sport), una reseña de las leyes y normas pertinentes sobre el soborno en el deporte (Tackling Bribery in Sport: An Overview of Relevant Laws and Standards), una guía de recursos sobre los enfoques jurídicos para hacer frente a la manipulación de las competiciones deportivas (Legal Approaches to Tackling the Manipulation of Sports Competitions), un informe sobre la prevención y la lucha contra la corrupción en lo relativo a los delitos que repercutían en el medio ambiente (Preventing and Combating Corruption as it Relates to Crimes that Have an Impact on the Environment: An Overview), un informe sobre los principales agentes, estructuras organizativas y modelos de negocios vinculados a los delitos contra la fauna y flora silvestre (“Wildlife crime: key actors, organizational structures and business models”), y documentos sobre las experiencias y lecciones aprendidas en materia de crisis y corrupción a partir de las respuestas de emergencia durante la COVID-19 (“Crises and corruption: emergency responses during COVID-19: experiences and lessons learned”), la forma en que la pandemia había exacerbado la amenaza que suponía la corrupción para los derechos humanos y el desarrollo sostenible en los países insulares del Pacífico (The COVID-19 Pandemic: Exacerbating the Threat of Corruption to Human Rights and Sustainable Development in Pacific Island Countries) y los riesgos de corrupción en la contratación pública en el contexto de la COVID-19 en los países insulares del Pacífico (“Corruption risks in public procurement in the context of COVID-19 in Pacific island countries”), así como un documento de políticas para la totalidad del sistema de las Naciones Unidas sobre los desafíos relacionados con la corrupción y la COVID-19 en la respuesta a las crisis y la recuperación posterior (“Corruption and COVID-19: challenges in crisis response and recovery”), elaborado por el Equipo de Tareas Mundial sobre Corrupción bajo la dirección conjunta de la UNODC, el Departamento de Asuntos Políticos y de Consolidación de la Paz de la Secretaría y el PNUD.'],\n",
       " 'token_total': [501, 511, 511, 504],\n",
       " 'split_symbol': ['\\n', '\\n\\n', '\\n\\n', '\\n\\n'],\n",
       " 'record': ['A_CN.9_WG.III_WP.216',\n",
       "  'CTOC_COP_2022_4',\n",
       "  'A_CN.9_WG.I_WP.128',\n",
       "  'CAC_COSP_WG.4_2022_5'],\n",
       " 'source': ['ru', 'fr', 'es', 'es'],\n",
       " 'target': ['en', 'en', 'en', 'en']}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0: 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b20935-5c96-4da2-a100-36b579c75bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
